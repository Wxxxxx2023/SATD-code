{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing and identification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-collaboration",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-20T21:42:57.622078Z",
     "end_time": "2023-04-20T21:42:58.409262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ant_processing.csv\n",
      "Col_processing.csv\n",
      "EMF_processing.csv\n",
      "Hib_processing.csv\n",
      "JEd_processing.csv\n",
      "JFr_processing.csv\n",
      "JMe_processing.csv\n",
      "JRu_processing.csv\n",
      "SQu_processing.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\app\\anaconda3\\envs\\py37\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "D:\\app\\anaconda3\\envs\\py37\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9590\n",
      "recall: 0.8853503184713376\n",
      "precision: 0.8481355932203389\n",
      "f1: 0.8663434903047091\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "text_neg = []\n",
    "text_pos = []\n",
    "\n",
    "\n",
    "def loaddata(path):\n",
    "    numwords = []\n",
    "    file = open(path, encoding=\"utf-8\")\n",
    "    texts_projects = []\n",
    "    label_projects = []\n",
    "    for line in file.readlines():\n",
    "        line = line.strip(\"\\n\")\n",
    "        a = line[13:].lower()\n",
    "        if a == '':\n",
    "            continue\n",
    "        if a == '   ':\n",
    "            continue\n",
    "        if a == ' ':\n",
    "            continue\n",
    "        if a == '  ':\n",
    "            continue\n",
    "        texts_projects.append(a)\n",
    "\n",
    "        counter = len(a.split(\" \"))\n",
    "        numwords.append(counter)\n",
    "        if line[4:12] == \"negative\":\n",
    "            label_projects.append(0)\n",
    "            text_neg.append(a)\n",
    "        else:\n",
    "            label_projects.append(1)\n",
    "            text_pos.append(a)\n",
    "    return texts_projects, label_projects, numwords\n",
    "\n",
    "\n",
    "def dataSplit(target):\n",
    "    path = './data'\n",
    "    files = os.listdir(path)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    numwords = []\n",
    "    for file in files:\n",
    "        if file != target:\n",
    "            print(file)\n",
    "            text, label, numword = loaddata(r'./data/' + file)\n",
    "            texts += text\n",
    "            labels += label\n",
    "            numwords += numword\n",
    "    text1, label1, numwords1 = loaddata(r'./data/' + target)\n",
    "    return texts, labels, numwords, text1, label1, numwords1\n",
    "\n",
    "\n",
    "target = 'Arg_processing.csv'\n",
    "vocabulary = {\"this\": 0, \"the\": 1, \"not\": 2}\n",
    "x_tr, y_train, numwords, x_te, y_test, numwords1 = dataSplit(target)\n",
    "idf = TfidfVectorizer(lowercase=False, max_features=1000, stop_words='english')\n",
    "x_train = idf.fit_transform(x_tr)\n",
    "x_test = idf.transform(x_te)\n",
    "smo = BorderlineSMOTE()\n",
    "x_train, y_train = smo.fit_resample(x_train, y_train)\n",
    "smo.fit(x_train, y_train)\n",
    "from lightgbm import LGBMClassifier\n",
    "# gbm0=LGBMClassifier(max_depth= 4, num_leaves= 50,learning_rate=0.1, n_estimators=800)\n",
    "gbm0=LGBMClassifier(max_depth= 3, num_leaves= 50,learning_rate=0.1, n_estimators=800)\n",
    "eval_set = [(x_test, y_test)]\n",
    "gbm0.fit(x_train, y_train, eval_set=[(x_test, y_test)])\n",
    "# gbm0=GradientBoostingClassifier(n_estimators=80, max_depth=6, min_samples_split =1400)\n",
    "y_predict = gbm0.predict(x_test)\n",
    "y_pred = gbm0.predict(x_test)\n",
    "y_predprob = gbm0.predict_proba(x_test)[:, 1]\n",
    "print(\"Accuracy:%.4f\" % metrics.accuracy_score(y_test, y_pred))\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_predict).ravel()\n",
    "rec = tp / (tp + fn)\n",
    "Recall = tp / (tp + fn)\n",
    "specificity = tn / (fp + tn)\n",
    "precision = tp / (tp + fp)\n",
    "gmeans = math.sqrt(specificity * rec)\n",
    "f1 = 2 * precision * rec / (precision + rec)\n",
    "\n",
    "print(\"recall:\", rec)\n",
    "\n",
    "print(\"precision:\", precision)\n",
    "\n",
    "print(\"f1:\", f1)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LIME"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "british-restoration",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-20T21:42:58.393259Z",
     "end_time": "2023-04-20T21:42:58.409262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.10757569, 0.89242431]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "gbm0.predict_proba(x_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operational-radius",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24772\\2037035518.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mlime\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlime_text\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpipeline\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmake_pipeline\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_pipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgbm0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mx_te\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "c = make_pipeline(idf, gbm0)\n",
    "print(c.predict_proba([x_te[0]]))\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names = ['negative', 'positive']\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "idx = 8000\n",
    "exp = explainer.explain_instance(x_te[idx], c.predict_proba, num_features=6)\n",
    "print('Document id: %d' % idx)\n",
    "print(c.predict_proba([x_te[idx]]))\n",
    "print('Probability =', c.predict_proba([x_te[idx]])[0, 1])\n",
    "print('True class: %s' % class_names[y_test[idx]])\n",
    "exp.as_list()\n",
    "print(exp.as_list())\n",
    "\n",
    "print('Original prediction:', gbm0.predict_proba(x_test[idx])[0, 1])\n",
    "tmp = x_test[idx].copy()\n",
    "tmp[0, idf.vocabulary_['todo']] = 0\n",
    "print('Prediction removing some features:', gbm0.predict_proba(tmp)[0, 1])\n",
    "print('Difference:', gbm0.predict_proba(tmp)[0, 1] - gbm0.predict_proba(x_test[idx])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.save_to_file('./tmp/oi.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
